model: topk
ckpt_path: null
num_sampled_tactics: 64

distributed: true
gpu_per_process: 0.45
cpu_per_process: 0.25

diversity_config:
  # whether to use topk or random filtering
  random: true
  # number of tactics filtered to (K)
  num_filtered: 32

config:
  model_name: pretrained_tactic_generator_path
  #   retriever args
  ret_ckpt_path: runs/retriever_novel_premises.ckpt
  #  indexed_corpus_path: runs/indexed_corpus
  indexed_corpus_path: runs/indexed_corpus_minif2f
  eval_num_retrieved: 100

  lr: 5e-6
  warmup_steps: 200
  length_penalty: 0.0
  num_beams: 64
  max_seq_len: 2300
  gen_config:
    strategy: beam
    length_penalty: 0.0
  # configuration for the eval loop in training (terminates based on live proving performance)
  eval_config:
    eval_num_theorems: 200
    shuffle: false
    timeout: 600
